{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Scikit-learn - NLP (Natural Language Processing)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Understand and create an ML pipeline for NLP (Natural Language Processing)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\mikee\\\\Desktop\\\\ML_practice\\\\jupyter_notebooks'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You set a new current directory\n"
          ]
        }
      ],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\mikee\\\\Desktop\\\\ML_practice'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "## NLP (Natural Language Processing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "Conversational language is unlike text neatly entered into form inputs. Instead it is unstructured data that cannot be neatly broken down into elements in a row-column database table; there is a vast quantity of information available within it and waiting to be accessed.\n",
        "\n",
        "* Therefore, natural language processing aims to gather, extract and make available all of this information.\n",
        "\n",
        "NLP is not a trivial task since its goal is to understand the language and not only process the text/strings/keywords.\n",
        "\n",
        "* As we know, language is ambiguous, subjective and subtle. New words and terms are constantly added/updated, and their meaning may change according to the context.\n",
        "* These aspects all together make NLP a very interesting and challenging task for ML.\n",
        "\n",
        "We will study NLP (Natural Language Processing) as a supervised learning approach where the features are text, and the target variable is a meaning associated with that given text. Therefore, the ML task is Classification.\n",
        "\n",
        "* Therefore, the workflow will be similar to what we covered for Classification tasks, where we:\n",
        "    * Load the data\n",
        "    * Define the pipeline steps\n",
        "    * Split the data into train and test sets\n",
        "    * Train multiple pipelines using hyperparameter optimisation\n",
        "    * Evaluate pipeline performance\n",
        "* One difference will be defining the pipeline steps, where we will use steps for pre-processing the textual data before the modelling stage. Once you have a processed text, you can use ML algorithms to predict your target variable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load data\n",
        "\n",
        "We will use a dataset that contains records telling if a given SMS message is spam or not (spam or ham). We load the data from GitHub.\n",
        "\n",
        "In this project, we are interested in predicting if a given message is spam or not; therefore, the ML task is Classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1672, 2)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1337</th>\n",
              "      <td>spam</td>\n",
              "      <td>Someone U know has asked our dating service 2 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>ham</td>\n",
              "      <td>I'm home. Doc gave me pain meds says everythin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1548</th>\n",
              "      <td>ham</td>\n",
              "      <td>Feb  &amp;lt;#&amp;gt;  is \"I LOVE U\" day. Send dis to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2603</th>\n",
              "      <td>ham</td>\n",
              "      <td>Just finished. Missing you plenty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1966</th>\n",
              "      <td>ham</td>\n",
              "      <td>Hello. Sort of out in town already. That . So ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     label                                            message\n",
              "1337  spam  Someone U know has asked our dating service 2 ...\n",
              "568    ham  I'm home. Doc gave me pain meds says everythin...\n",
              "1548   ham  Feb  &lt;#&gt;  is \"I LOVE U\" day. Send dis to...\n",
              "2603   ham                  Just finished. Missing you plenty\n",
              "1966   ham  Hello. Sort of out in town already. That . So ..."
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "url = 'https://raw.githubusercontent.com/ShresthaSudip/SMS_Spam_Detection_DNN_LSTM_BiLSTM/master/SMSSpamCollection'\n",
        "df = (pd.read_csv(url, sep ='\\t',names=[\"label\", \"message\"])\n",
        "    .sample(frac=0.6, random_state=0)\n",
        "    .reset_index(drop=True)\n",
        "    )\n",
        "df = df.sample(frac=0.5, random_state=101)\n",
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "#### Split Data\n",
        "\n",
        "As usual, we are splitting the data into train and test sets.\n",
        "\n",
        "* In this case, the dataset has two columns containing the message text, and the label tells whether the SMS message was spam or not.\n",
        "* In the end, we have a Pandas Series for the features (message) and target (label) - note the brackets subsetting the data, for example, df['message']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1337,) (1337,) (335,) (335,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['message'], df['label'],\n",
        "                                                    test_size=0.2, random_state=101)\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create the pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will consider steps for (1) cleaning the textual data and (2) representing the text as numbers or feature extraction.\n",
        "\n",
        "* (1) In our case, we will make the text lowercase and remove punctuation for text cleaning.\n",
        "\n",
        "    * The practical tasks for cleaning the textual data will differ from dataset to dataset; for example, you may have a dataset where you need to clean HTML tags, so you need a function to do that for you, or eventually, you need to remove diacritics (marks located above or below a letter to reflect a particular pronunciation, like resumé)\n",
        "* (2) There are also multiple techniques for feature extraction; we will consider the ones we covered in Module 2; in this case, we will tokenise the text and then use TF-IDF (Term Frequency－Inverse Document Frequency)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are using texthero (TextHero does not work with Python 3.12) so we use pandas and regex instead.\n",
        "\n",
        "*  We need to create a custom Python class to integrate into the pipeline. This is a task that requires expertise and understanding, and it's a key step in ensuring our custom transformer is seamlessly added to the ML pipeline. We are using the same approach for creating custom transformers we saw in the feature-engine lesson, where we use BaseEstimator and TransformerMixin and create fit and transform methods. So the custom transformer can be added correctly to the ML pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "class text_cleaning(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Lowercase and remove punctuation using pandas and regex\n",
        "        return X.apply(lambda s: re.sub(r'[^\\w\\s]', '', str(s).lower()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For feature extraction, we use CountVectorizer and TfidfTransformer. You can find their documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and h[ere](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html).\n",
        "\n",
        "* We need to convert the textual data to a format from which the algorithms can learn the relationships, also known as vectors.\n",
        "* CountVectorizer: According to its documentation, it converts a collection of text documents to a matrix of token counts. It stores the number of times every word is used in our text data. We are also removing English \"stop words\".\n",
        "* (TfidfTransformer) Term Frequency－Inverse Document Frequency Transformer: It transforms a count matrix to a normalised tf or tf-idf representation according to its documentation. The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and are empirically less informative than features that occur in a small fraction of the data. In addition, this highlights the words that are unique to a document, thus better for characterising it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our pipeline will have four steps:\n",
        "\n",
        "* Text cleaning: lowercase the text and remove punctuation\n",
        "* CountVectorizer: convert text to token\n",
        "* TF-IDF: transform a count matrix to a normalised tf or tf-idf representation\n",
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "def PipelineOptimization(model):\n",
        "  pipeline = Pipeline([\n",
        "                       \n",
        "        ( 'text_cleaning', text_cleaning() ),\n",
        "        ( 'vect', CountVectorizer(stop_words='english') ),\n",
        "        ( 'tfidf', TfidfTransformer() ),\n",
        "        ( 'model', model )\n",
        "    ])\n",
        "  \n",
        "  return pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We load the Python class (HyperparameterOptimizationSearch) which aims to fit a set of algorithms with multiple hyperparameters. A quick reminder of what this class does:\n",
        "\n",
        "* We define a set of algorithms and their respective hyperparameter values.\n",
        "* The code iterates on each algorithm and fits pipelines using GridSearchCV, considering its respective hyperparameter values. The result is stored. This process is repeated for all algorithms that the user listed.\n",
        "* Once all pipelines are trained, the developer can retrieve a list with a performance result summary and an object that contains all trained pipelines. The developer can then subset the best pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "class HyperparameterOptimizationSearch:\n",
        "\n",
        "    def __init__(self, models, params):\n",
        "        self.models = models\n",
        "        self.params = params\n",
        "        self.keys = models.keys()\n",
        "        self.grid_searches = {}\n",
        "\n",
        "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
        "        for key in self.keys:\n",
        "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
        "            model=  PipelineOptimization(self.models[key])\n",
        "\n",
        "            params = self.params[key]\n",
        "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs, verbose=verbose, scoring=scoring)\n",
        "            gs.fit(X,y)\n",
        "            self.grid_searches[key] = gs    \n",
        "\n",
        "    def score_summary(self, sort_by='mean_score'):\n",
        "        def row(key, scores, params):\n",
        "            d = {\n",
        "                 'estimator': key,\n",
        "                 'min_score': min(scores),\n",
        "                 'max_score': max(scores),\n",
        "                 'mean_score': np.mean(scores),\n",
        "                 'std_score': np.std(scores),\n",
        "            }\n",
        "            return pd.Series({**params,**d})\n",
        "\n",
        "        rows = []\n",
        "        for k in self.grid_searches:\n",
        "            params = self.grid_searches[k].cv_results_['params']\n",
        "            scores = []\n",
        "            for i in range(self.grid_searches[k].cv):\n",
        "                key = \"split{}_test_score\".format(i)\n",
        "                r = self.grid_searches[k].cv_results_[key]        \n",
        "                scores.append(r.reshape(len(params),1))\n",
        "\n",
        "            all_scores = np.hstack(scores)\n",
        "            for p, s in zip(params,all_scores):\n",
        "                rows.append((row(k, s, p)))\n",
        "\n",
        "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
        "\n",
        "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
        "        columns = columns + [c for c in df.columns if c not in columns]\n",
        "\n",
        "        return df[columns], self.grid_searches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### List Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we list the algorithms we want to use for this task. First, we are considering new estimators from Scikit-learn that typically offer reasonable performance for NLP tasks.\n",
        "\n",
        "* It doesn't mean we couldn't have considered the algorithms we have seen already in the course, like tree-based algorithms. However, the central aspect is that we should use algorithms that are proven to be more effective for NLP tasks, giving you a solid foundation for your learning.\n",
        "* We will consider four algorithms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "models_search = {\n",
        "    \"MultinomialNB\":MultinomialNB(),\n",
        "    \"SGDClassifier\":SGDClassifier(random_state=101),\n",
        "    \"SVC\": SVC(random_state=101),\n",
        "    \"LinearSVC\": LinearSVC(random_state=101),\n",
        "}\n",
        "\n",
        "\n",
        "params_search = {\n",
        "   \"MultinomialNB\":{},\n",
        "    \"SGDClassifier\": {},\n",
        "   \"SVC\": {},\n",
        "    \"LinearSVC\": {},\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Fit multiple pipelines with multiple algorithms using their default hyperparameters\n",
        "\n",
        "We start by fitting multiple pipelines using the default hyperparameters.\n",
        "\n",
        "We pass in the training data, set the scoring metric to accuracy (we assume our stakeholders are interested in how accurate their system is) and set cv=4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Running GridSearchCV for MultinomialNB \n",
            "\n",
            "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
            "\n",
            "Running GridSearchCV for SGDClassifier \n",
            "\n",
            "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
            "\n",
            "Running GridSearchCV for SGDClassifier \n",
            "\n",
            "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
            "\n",
            "Running GridSearchCV for SVC \n",
            "\n",
            "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
            "\n",
            "Running GridSearchCV for SVC \n",
            "\n",
            "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
            "\n",
            "Running GridSearchCV for LinearSVC \n",
            "\n",
            "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
            "\n",
            "Running GridSearchCV for LinearSVC \n",
            "\n",
            "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mikee\\Desktop\\ML_practice\\.venv\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "search = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
        "search.fit(X_train, y_train,\n",
        "           scoring='accuracy',\n",
        "           n_jobs=-2,\n",
        "           cv=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now check the training results summary.\n",
        "\n",
        "Note that SGDClassifier performed best, and the difference to LinearSVC is slight; both are close. The other algorithms also perform well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>estimator</th>\n",
              "      <th>min_score</th>\n",
              "      <th>mean_score</th>\n",
              "      <th>max_score</th>\n",
              "      <th>std_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SGDClassifier</td>\n",
              "      <td>0.970149</td>\n",
              "      <td>0.97607</td>\n",
              "      <td>0.98503</td>\n",
              "      <td>0.005577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LinearSVC</td>\n",
              "      <td>0.958084</td>\n",
              "      <td>0.968587</td>\n",
              "      <td>0.979042</td>\n",
              "      <td>0.00748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SVC</td>\n",
              "      <td>0.92515</td>\n",
              "      <td>0.931185</td>\n",
              "      <td>0.937313</td>\n",
              "      <td>0.004301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MultinomialNB</td>\n",
              "      <td>0.92515</td>\n",
              "      <td>0.930443</td>\n",
              "      <td>0.934132</td>\n",
              "      <td>0.003859</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       estimator min_score mean_score max_score std_score\n",
              "1  SGDClassifier  0.970149    0.97607   0.98503  0.005577\n",
              "3      LinearSVC  0.958084   0.968587  0.979042   0.00748\n",
              "2            SVC   0.92515   0.931185  0.937313  0.004301\n",
              "0  MultinomialNB   0.92515   0.930443  0.934132  0.003859"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
        "grid_search_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv (3.12.8)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
